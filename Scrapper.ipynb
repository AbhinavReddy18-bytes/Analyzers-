{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nl6jnn-7bDLm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Dqjaw-TGYWXa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_URL = 'https://law.justia.com'\n",
    "OUTFILE = 'penal_codes.csv'\n",
    "OUTSIZE = 1 * 1024 * 1024 * 1024  # 1GB\n",
    "COLUMN_LABELS = 'id, citation\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xiromSn8YXsc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_soup_from_url(url, timeout=10):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=timeout)\n",
    "        res.raise_for_status()\n",
    "        return BeautifulSoup(res.text, 'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DHWdq7XBYZVc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_code_contents_from_url(url):\n",
    "    soup = get_soup_from_url(url)\n",
    "    if soup:\n",
    "        content_paragraphs = soup.select('p')\n",
    "        code_text = \" \".join(paragraph.getText().strip().replace(\"\\r\", \"\").replace(\"\\n\", \" \").replace(\"  \", \" \").lower() for paragraph in content_paragraphs)\n",
    "        return code_text\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Nc_eAQevYbWX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_links_from_url(url):\n",
    "    soup = get_soup_from_url(url)\n",
    "    if soup:\n",
    "        code_listing = soup.select('.codes-listing')\n",
    "        if not code_listing:\n",
    "            print('Found base page')\n",
    "            return []\n",
    "        links = BeautifulSoup(str(code_listing[0]), 'html.parser').select('a')\n",
    "        print(f'Found {len(links)} links')\n",
    "        return links\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Zt1xrlxZYcti",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_column_labels_once(outfile):\n",
    "    if not os.path.exists(outfile):\n",
    "        with open(outfile, 'w') as csvfile:\n",
    "            csvfile.write(COLUMN_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gztQbo0oYgHW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def append_data_to_csv(data, outfile):\n",
    "    with open(outfile, 'a') as csvfile:\n",
    "        csvfile.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SjaszBM7YhUN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_justia_penal_codes():\n",
    "    size = 0\n",
    "    write_column_labels_once(OUTFILE)\n",
    "    titles = get_links_from_url(f'{BASE_URL}/codes/new-york/2018/pen/part-3/')\n",
    "\n",
    "    for i, title in enumerate(titles):\n",
    "        titleUrl = f'{BASE_URL}{title.get(\"href\")}'\n",
    "        articles = get_links_from_url(titleUrl)\n",
    "\n",
    "        for j, article in enumerate(articles):\n",
    "            if size >= OUTSIZE:\n",
    "                break\n",
    "\n",
    "            articleUrl = f'{BASE_URL}{article.get(\"href\")}'\n",
    "            citations = get_links_from_url(articleUrl)\n",
    "\n",
    "            for k, citation in enumerate(citations):\n",
    "                if size >= OUTSIZE:\n",
    "                    break\n",
    "\n",
    "                id_text = f'Title {i} Article {j} Citation {k}'\n",
    "                citationUrl = f'{BASE_URL}{citation.get(\"href\")}'\n",
    "                code_content = get_code_contents_from_url(citationUrl).replace(\",\", \"\")\n",
    "                txt = f'{id_text},{code_content}\\n'\n",
    "                size += len(txt)\n",
    "                append_data_to_csv(txt, OUTFILE)\n",
    "\n",
    "    print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZq8J39pYn8Q",
    "outputId": "ac008258-056f-436b-ab05-9633d728653e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 links\n",
      "Found 4 links\n",
      "Found 7 links\n",
      "Found 10 links\n",
      "Found 3 links\n",
      "Found 7 links\n",
      "Found 5 links\n",
      "Found 30 links\n",
      "Found 4 links\n",
      "Found 14 links\n",
      "Found 28 links\n",
      "Found 18 links\n",
      "Found 3 links\n",
      "Found 10 links\n",
      "Found 21 links\n",
      "Found 6 links\n",
      "Found 5 links\n",
      "Found 12 links\n",
      "Found 12 links\n",
      "Found 11 links\n",
      "Found 4 links\n",
      "Found 28 links\n",
      "Found 10 links\n",
      "Found 19 links\n",
      "Found 10 links\n",
      "Found 15 links\n",
      "Found 7 links\n",
      "Found 7 links\n",
      "Found 5 links\n",
      "Found 20 links\n",
      "Found 4 links\n",
      "Found 7 links\n",
      "Found 35 links\n",
      "Found 5 links\n",
      "Found 14 links\n",
      "Found 19 links\n",
      "Found 15 links\n",
      "Found 11 links\n",
      "Found 35 links\n",
      "Found 5 links\n",
      "Found 31 links\n",
      "Found 12 links\n",
      "Found 19 links\n",
      "Found 25 links\n",
      "Found 11 links\n",
      "Found 5 links\n",
      "Found 34 links\n",
      "Found 2 links\n",
      "Found 4 links\n",
      "Found 7 links\n",
      "Found 14 links\n",
      "Found 3 links\n",
      "Found 10 links\n",
      "Found 16 links\n",
      "Found 9 links\n",
      "Found 3 links\n",
      "Found 29 links\n",
      "Found 8 links\n",
      "Found 13 links\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "get_justia_penal_codes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDVHWkIBYoih"
   },
   "source": [
    "#Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "epNVpeOpdSpP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('penal_codes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1IPQe-IdZM8",
    "outputId": "0d0f00e5-4d82-4bed-b995-c1be5f4bc4fa",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             id  \\\n",
      "0  Title 0 Article 0 Citation 0   \n",
      "1  Title 0 Article 0 Citation 1   \n",
      "2  Title 0 Article 0 Citation 2   \n",
      "3  Title 0 Article 0 Citation 3   \n",
      "4  Title 0 Article 0 Citation 4   \n",
      "\n",
      "                                            citation  \n",
      "0  a person is guilty of criminal solicitation in...  \n",
      "1  a person is guilty of criminal solicitation in...  \n",
      "2  a person is guilty of criminal solicitation in...  \n",
      "3  a person is guilty of criminal solicitation in...  \n",
      "4  a person is guilty of criminal solicitation in...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "BU3A-3Gndtn9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1c802b240546c7aacef9e3eed1a506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe8934e98f44354b1edb48bf74f993e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76304fe99eb40c0ac5f822ad55eadcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0c1d84325e484b875b61afe161c6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('penal_codes.csv')\n",
    "\n",
    "# Load pre-trained model tokenizer (you may need to adjust the model name as required)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embeddings for a given text\n",
    "def get_bert_embedding(text):\n",
    "    # Tokenize and encode the text for BERT\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the mean of the last hidden state as the sentence embedding\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Ensure CUDA is available for PyTorch, if you have a GPU, to speed up processing\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "\n",
    "# Apply the BERT embedding function to the 'citation' column\n",
    "# If your dataset is large, consider processing in batches or using tqdm to track progress\n",
    "df['citation_embeddings'] = df[' citation'].apply(lambda x: get_bert_embedding(x)[:512])\n",
    "\n",
    "# Now df['citation_embeddings'] will contain the embeddings for each citation text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72ylv6ydeKoF",
    "outputId": "cdddc3e1-9f9e-426e-f69f-3477567a8724",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [-0.045585606, -0.1362382, 0.39512724, -0.1538...\n",
       "1      [-0.22505607, -0.008862062, 0.29037467, -0.255...\n",
       "2      [-0.11252634, -0.08307119, 0.43694556, -0.2283...\n",
       "3      [-0.13397354, -0.18946846, 0.39071646, -0.1566...\n",
       "4      [-0.14979129, -0.07455121, 0.43193123, -0.2217...\n",
       "                             ...                        \n",
       "687    [-0.10705753, -0.009622224, 0.23218858, -0.153...\n",
       "688    [-0.09208092, -0.0058112266, 0.20358907, -0.12...\n",
       "689    [-0.15642193, 0.15235814, 0.2109673, -0.140971...\n",
       "690    [-0.16718373, -0.023032503, 0.16628788, -0.114...\n",
       "691    [-0.067314036, 0.062474437, 0.35778087, -0.096...\n",
       "Name: citation_embeddings, Length: 692, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['citation_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CAI6307_PROJECTS",
   "language": "python",
   "name": "cai6307_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
